Oh, Those cooky mathematicians!

"Inventing" or "discovering" (which can be debated) facts about the world
mathematics. For much of the history of mathematics learnings were made to
fill a need. Have a client that owes you a lot of money? Well, negative numbers
might be good for tracking that business relationship!

However, digressions about the world of business aside. I am here to tell a
story. It's a story about how mathematics became something more than a niche
hobby for aristocrats and became something AUTOMATED so that everyone is
allowed to be as ignorant as they please of how to define a limit or the right
situations to apply the arc tangent function. Now everyone is at all times
licensed to be as ignorant of the bytes running the simulation because now
most people carry a super computer in their pocket. üòÆ‚Äçüí®

## üöÄ Let's hit the wayback machine!

Let's climb aboard the wayback machine. Setting the dial for way-way-back!
Back to a time before Vercel and Next.js, back to a time before React. Back to a
time when names were important and discoveries were not made my massive
corporations but individuals with way too much time on their hands.

Oh there's Mike Bostock writing his white paper on D3! There's Mr. Doob and
creating Three.js to bring demo scene to the web. But we're not going to linger
here we're going to the way back. We're going back through 2009. We just passed
Why the Lucky Stiff writing metaid to throw a wrench in the cogs of the Rails
community! Oh there's John Resig formulating a way to make browser quirks a
thing of the past.

There's Frank Deremer writing his 1969 PhD dissertation on the Look Ahead Left
to Right (LALR) parsing algorithm. A parsing algorithm that would be used in
almost every project until the early 2000s. There's Donald Knuth at Bell
Laboratories (author of The Art of Computer Programming) writing the original
research on LR parsing algorithmns. There's Noam Chomsky also at Bell Labs
formulating the Chomsky heirarchy of languages that would help to define
formal grammars as seperate and distinct from natural language.

## ü¶ï We are passing into the digital paleolithic

Here are the US and Soviet Russia in a computing race. The Soviet's just
discovered the balanced ternary numbering system. An innovation that would have
opened the door to more efficient computers and chemical computing... Ah well
not every innovation gains footing. Here we are watching the first magnetic tape
reals put up on main frames that take up an entire building.

Now we aren't going all the way back. The webwork of connections that formulated
the origins of computer science goes as far back as the discovery (or invention)
of the natural numbers. We are going to pause at a period in the 1940s just
prior to the start of the WWII.

Keep in mind that back this far back computing exists only in the imagination.
The early writings on computing were understandably thought experiments. There
was no hardware realized to prove the veracity of this technical thinking.

It all started with Mr. Alan Turing or rather his faculty advisor Alonzo
Church. Turing a graduate student at the time collaborated with his mentor
Alonzo Church. Alonzo had already developed a theory of computation which was
encoded in his definition of the lambda calculus.

Turing who by some strange twist of fate was able to see the applications of
this theoretical model of computation devised a separete alternative model the
"Turing Machine". A Turing Machine consists of an infinitely long tape and the
ability to do four operations.

1. Advance the tape
2. Rewind the tape
3. Write a bit
4. Read a bit

Alonzo church had already completed the first description of what would later
be termed a "Turing Complete" system in his work on the Lamda Calculus. As
Turing's fatulty advisor they both got the win for publishing a universal
model of computation. Alonzo had piorneered the idea and Turing had
visualized it for a non-academic population.

I studied parser theory in depth in my under-graduate studies. I was primarily
interested in Generalized Left to Right (GLR) parsers that have the capaticy to
parse natural languages in some contexts. I was unaware at the time of all of
the developments in machine learning, which would come to be called data
science, which would come to be called AI.

In any case I spent a lot of time focused on parser theory and the fomulating
ideas around it. Parser theory was always something I enjoyed because it meant
thinking like a machine for long periods of time to make any forward progress.

I worked on an individual learning contract in college at the Everegreen State
College implementing an Integrated Development Environment (IDE) in
JavaScript that levereaged the newly released Three.js library that
interfaced with WebGL and could produced interactive 3D visualizations. In the
past I had developed my own rudimentary 3d framework rendered in JavaScript
but without 3D hardware acceleration it had not been ready to put in front of
my commercial clients.

For my initial Individual Learning Contract at Evergreen I developed a grammar
loosely based off of Ruby, CoffeeScript, and NetLogo. A language focused on
creating 3D visualizations with the fewest lines of code possible. Like Why the
Lucky Stiff (\_why) I had been inspired by the idea of teaching children to code.

When I first started programming. There were many Apple IIe computers available
in public schools. The graphics system while extremely simplistic could have
you drawing graphics on the screen in two or three lines of code.

I had noticed as my career went on that just the boilerplate to render to the
screen had become unmanagble for a junior programmer. I wanted to create an
integrated development environment that would enable easy creation of
visualizations with an emphasis on making mathematical functions "first class
citizens" of the language.

I enjoyed that contract but was frustrated that I had leveraged a third party
parser generator library, Jison. I know nothing of Jison's internal workings
and new very little about parser theory.

I decided to do a second contract building my own parser generator from scratch.
At first I focused on implementing a GLR parser generator but realizing the
complexities of supporting the entire Unicode character set for a programming
language I settled for GLR. It was during this research that I first came upon
the literature related to Parsing Expression Grammars (PEG). At the time there
was no PEG implementations in JavaScript.

Despite my flickering interest in implementing a PEG in JavaScript I contiuned
down the path of creating an LALR parser generator in JavaScript. The parser
generator I developed had numerous optimizations that made it faster than the
current state of the art in JavaScript based parsers.

Little did I know that in several years that all of my work would be
Transpiled. And that my skills in raw JavaScript would become a vestige of the
past.

## üßë‚Äçüíª Coding for money

I worked for an employer that looked at patents as a way of verifying one's
worth to the organization. I was extremely resistent at first to the idea of
filing a patent. As a largely self taught software engineer I had always
appreciated the free nature of information on the internet. I had always
despised pay walls and walled gardens of content, to me these developments
had perverted the true nature of the world wide web envisioned by Time Berners
Lee.

I was convinced that filing a patent could possibly play in my favor and so set
about working on several patents related to our work with a coworker.
